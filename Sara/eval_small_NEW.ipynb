{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79a61cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -q \"chronos-forecasting>=2.0\" scikit-learn matplotlib pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "970fec2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/TheDatumOrg/TSB-AD.git\n",
      "  Cloning https://github.com/TheDatumOrg/TSB-AD.git to c:\\users\\sarac\\appdata\\local\\temp\\pip-req-build-yguskphi\n",
      "  Resolved https://github.com/TheDatumOrg/TSB-AD.git to commit 5e1d132ec3d9099eeaf9407c601004a0d2ec2d37\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: tqdm in c:\\python310\\lib\\site-packages (from TSB_AD==1.5) (4.67.1)\n",
      "Collecting torchinfo (from TSB_AD==1.5)\n",
      "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h5py (from TSB_AD==1.5)\n",
      "  Downloading h5py-3.15.1-cp310-cp310-win_amd64.whl.metadata (3.1 kB)\n",
      "Requirement already satisfied: einops in c:\\python310\\lib\\site-packages (from TSB_AD==1.5) (0.8.1)\n",
      "Collecting numpy<2.0,>=1.24.3 (from TSB_AD==1.5)\n",
      "  Downloading numpy-1.26.4-cp310-cp310-win_amd64.whl.metadata (61 kB)\n",
      "Collecting matplotlib>=3.7.5 (from TSB_AD==1.5)\n",
      "  Downloading matplotlib-3.10.8-cp310-cp310-win_amd64.whl.metadata (52 kB)\n",
      "Requirement already satisfied: pandas>=2.0.3 in c:\\python310\\lib\\site-packages (from TSB_AD==1.5) (2.2.3)\n",
      "Collecting arch>=5.3.1 (from TSB_AD==1.5)\n",
      "  Downloading arch-8.0.0-cp310-cp310-win_amd64.whl.metadata (13 kB)\n",
      "Collecting hurst>=0.0.5 (from TSB_AD==1.5)\n",
      "  Downloading hurst-0.0.5-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting tslearn>=0.6.3 (from TSB_AD==1.5)\n",
      "  Downloading tslearn-0.7.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting cython>=3.0.10 (from TSB_AD==1.5)\n",
      "  Downloading cython-3.2.4-cp310-cp310-win_amd64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: scikit-learn>=1.3.2 in c:\\python310\\lib\\site-packages (from TSB_AD==1.5) (1.7.2)\n",
      "Collecting stumpy>=1.12.0 (from TSB_AD==1.5)\n",
      "  Downloading stumpy-1.13.0-py3-none-any.whl.metadata (28 kB)\n",
      "Requirement already satisfied: networkx>=3.1 in c:\\python310\\lib\\site-packages (from TSB_AD==1.5) (3.1)\n",
      "Requirement already satisfied: transformers>=4.38.0 in c:\\python310\\lib\\site-packages (from TSB_AD==1.5) (4.57.3)\n",
      "Requirement already satisfied: torch>=1.8.0 in c:\\python310\\lib\\site-packages (from TSB_AD==1.5) (2.6.0)\n",
      "Requirement already satisfied: scipy>=1.8 in c:\\python310\\lib\\site-packages (from arch>=5.3.1->TSB_AD==1.5) (1.14.1)\n",
      "Requirement already satisfied: statsmodels>=0.13.0 in c:\\python310\\lib\\site-packages (from arch>=5.3.1->TSB_AD==1.5) (0.14.4)\n",
      "Requirement already satisfied: packaging in c:\\python310\\lib\\site-packages (from arch>=5.3.1->TSB_AD==1.5) (24.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\python310\\lib\\site-packages (from matplotlib>=3.7.5->TSB_AD==1.5) (1.0.6)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\python310\\lib\\site-packages (from matplotlib>=3.7.5->TSB_AD==1.5) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\python310\\lib\\site-packages (from matplotlib>=3.7.5->TSB_AD==1.5) (4.38.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\python310\\lib\\site-packages (from matplotlib>=3.7.5->TSB_AD==1.5) (1.4.4)\n",
      "Requirement already satisfied: pillow>=8 in c:\\python310\\lib\\site-packages (from matplotlib>=3.7.5->TSB_AD==1.5) (9.3.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\python310\\lib\\site-packages (from matplotlib>=3.7.5->TSB_AD==1.5) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\python310\\lib\\site-packages (from matplotlib>=3.7.5->TSB_AD==1.5) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\python310\\lib\\site-packages (from pandas>=2.0.3->TSB_AD==1.5) (2022.4)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\python310\\lib\\site-packages (from pandas>=2.0.3->TSB_AD==1.5) (2024.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\python310\\lib\\site-packages (from scikit-learn>=1.3.2->TSB_AD==1.5) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\python310\\lib\\site-packages (from scikit-learn>=1.3.2->TSB_AD==1.5) (3.5.0)\n",
      "Requirement already satisfied: numba>=0.57.1 in c:\\python310\\lib\\site-packages (from stumpy>=1.12.0->TSB_AD==1.5) (0.60.0)\n",
      "Requirement already satisfied: filelock in c:\\python310\\lib\\site-packages (from torch>=1.8.0->TSB_AD==1.5) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\python310\\lib\\site-packages (from torch>=1.8.0->TSB_AD==1.5) (4.12.2)\n",
      "Requirement already satisfied: jinja2 in c:\\python310\\lib\\site-packages (from torch>=1.8.0->TSB_AD==1.5) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\python310\\lib\\site-packages (from torch>=1.8.0->TSB_AD==1.5) (2025.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\python310\\lib\\site-packages (from torch>=1.8.0->TSB_AD==1.5) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\python310\\lib\\site-packages (from sympy==1.13.1->torch>=1.8.0->TSB_AD==1.5) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\python310\\lib\\site-packages (from transformers>=4.38.0->TSB_AD==1.5) (0.36.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\python310\\lib\\site-packages (from transformers>=4.38.0->TSB_AD==1.5) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\python310\\lib\\site-packages (from transformers>=4.38.0->TSB_AD==1.5) (2025.11.3)\n",
      "Requirement already satisfied: requests in c:\\python310\\lib\\site-packages (from transformers>=4.38.0->TSB_AD==1.5) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\python310\\lib\\site-packages (from transformers>=4.38.0->TSB_AD==1.5) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\python310\\lib\\site-packages (from transformers>=4.38.0->TSB_AD==1.5) (0.7.0)\n",
      "Requirement already satisfied: colorama in c:\\python310\\lib\\site-packages (from tqdm->TSB_AD==1.5) (0.4.6)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in c:\\python310\\lib\\site-packages (from numba>=0.57.1->stumpy>=1.12.0->TSB_AD==1.5) (0.43.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\python310\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.7.5->TSB_AD==1.5) (1.16.0)\n",
      "Requirement already satisfied: patsy>=0.5.6 in c:\\python310\\lib\\site-packages (from statsmodels>=0.13.0->arch>=5.3.1->TSB_AD==1.5) (1.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\python310\\lib\\site-packages (from jinja2->torch>=1.8.0->TSB_AD==1.5) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python310\\lib\\site-packages (from requests->transformers>=4.38.0->TSB_AD==1.5) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python310\\lib\\site-packages (from requests->transformers>=4.38.0->TSB_AD==1.5) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python310\\lib\\site-packages (from requests->transformers>=4.38.0->TSB_AD==1.5) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python310\\lib\\site-packages (from requests->transformers>=4.38.0->TSB_AD==1.5) (2022.9.24)\n",
      "Downloading arch-8.0.0-cp310-cp310-win_amd64.whl (937 kB)\n",
      "   ---------------------------------------- 0.0/937.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 937.9/937.9 kB 6.1 MB/s eta 0:00:00\n",
      "Downloading cython-3.2.4-cp310-cp310-win_amd64.whl (2.8 MB)\n",
      "   ---------------------------------------- 0.0/2.8 MB ? eta -:--:--\n",
      "   ---------------------- ----------------- 1.6/2.8 MB 7.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.8/2.8 MB 7.3 MB/s eta 0:00:00\n",
      "Downloading hurst-0.0.5-py3-none-any.whl (5.9 kB)\n",
      "Downloading matplotlib-3.10.8-cp310-cp310-win_amd64.whl (8.1 MB)\n",
      "   ---------------------------------------- 0.0/8.1 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 1.6/8.1 MB 7.7 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 3.1/8.1 MB 8.0 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 5.0/8.1 MB 8.0 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 6.6/8.1 MB 7.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.1/8.1 MB 7.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.1/8.1 MB 7.6 MB/s eta 0:00:00\n",
      "Downloading numpy-1.26.4-cp310-cp310-win_amd64.whl (15.8 MB)\n",
      "   ---------------------------------------- 0.0/15.8 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 1.6/15.8 MB 7.6 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 3.1/15.8 MB 7.7 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 3.9/15.8 MB 6.0 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 6.0/15.8 MB 7.4 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 7.9/15.8 MB 7.5 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 9.4/15.8 MB 7.5 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 11.0/15.8 MB 7.5 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 12.1/15.8 MB 7.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 14.2/15.8 MB 7.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.7/15.8 MB 7.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.8/15.8 MB 7.4 MB/s eta 0:00:00\n",
      "Downloading stumpy-1.13.0-py3-none-any.whl (176 kB)\n",
      "Downloading tslearn-0.7.0-py3-none-any.whl (372 kB)\n",
      "Downloading h5py-3.15.1-cp310-cp310-win_amd64.whl (2.9 MB)\n",
      "   ---------------------------------------- 0.0/2.9 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 1.6/2.9 MB 7.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.9/2.9 MB 7.3 MB/s eta 0:00:00\n",
      "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
      "Building wheels for collected packages: TSB_AD\n",
      "  Building wheel for TSB_AD (pyproject.toml): started\n",
      "  Building wheel for TSB_AD (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for TSB_AD: filename=tsb_ad-1.5-py3-none-any.whl size=171102 sha256=d1445998920ad0d6959dcb21bda795bcccc548422025276f9e19cef7783879aa\n",
      "  Stored in directory: C:\\Users\\sarac\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-3_ykh7kn\\wheels\\40\\ec\\67\\19318250ce7e638a79997a8df43f70b69d03867f0c6b88e3a0\n",
      "Successfully built TSB_AD\n",
      "Installing collected packages: torchinfo, numpy, cython, h5py, stumpy, matplotlib, hurst, tslearn, arch, TSB_AD\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.23.5\n",
      "    Uninstalling numpy-1.23.5:\n",
      "      Successfully uninstalled numpy-1.23.5\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.6.2\n",
      "    Uninstalling matplotlib-3.6.2:\n",
      "      Successfully uninstalled matplotlib-3.6.2\n",
      "Successfully installed TSB_AD-1.5 arch-8.0.0 cython-3.2.4 h5py-3.15.1 hurst-0.0.5 matplotlib-3.10.8 numpy-1.26.4 stumpy-1.13.0 torchinfo-1.8.0 tslearn-0.7.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/TheDatumOrg/TSB-AD.git 'C:\\Users\\sarac\\AppData\\Local\\Temp\\pip-req-build-yguskphi'\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Python310\\Lib\\site-packages\\~umpy'.\n",
      "  You can safely remove it manually.\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/TheDatumOrg/TSB-AD.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b292664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2026-01-23 08:59:25--  https://www.thedatum.org/datasets/TSB-AD-U.zip\n",
      "Resolving www.thedatum.org (www.thedatum.org)... 69.163.141.146\n",
      "Connecting to www.thedatum.org (www.thedatum.org)|69.163.141.146|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 72900868 (70M) [application/zip]\n",
      "Saving to: ‘TSB-AD-U.zip’\n",
      "\n",
      "TSB-AD-U.zip        100%[===================>]  69.52M  19.5MB/s    in 4.7s    \n",
      "\n",
      "2026-01-23 08:59:30 (14.9 MB/s) - ‘TSB-AD-U.zip’ saved [72900868/72900868]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Se non l'hai ancora fatto:\n",
    "!wget https://www.thedatum.org/datasets/TSB-AD-U.zip\n",
    "!unzip -q TSB-AD-U.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d058fd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import torch\n",
    "import os\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from chronos import Chronos2Pipeline\n",
    "from TSB_AD.evaluation.metrics import get_metrics\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "\n",
    "from json import dump as json_dump, load as json_load\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d20a7ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pipeline(model_name: str = \"autogluon/chronos-2-small\", device: str = \"cuda\"):\n",
    "    \"\"\"Load Chronos-2 pipeline\"\"\"    \n",
    "    return Chronos2Pipeline.from_pretrained(model_name, device_map=\"cuda\" if device and torch.cuda.is_available() else \"cpu\", torch_dtype=torch.float32)\n",
    "\n",
    "\n",
    "def get_timestamp(start_date: str = \"2026-01-01 00:00:00\", periods: int = 100, freq: str = 'min'):\n",
    "    \"\"\"Generate timestamps for time series\"\"\"\n",
    "    return pd.date_range(start=start_date, periods=periods, freq=freq)\n",
    "\n",
    "\n",
    "def prepare_data_for_chronos(dataset_path: str):\n",
    "    \"\"\"\n",
    "    Prepare data in Chronos-2 format (DataFrame with timestamp, item_id, target columns)\n",
    "    \n",
    "    Returns:\n",
    "        - time_series_df: Formatted DataFrame for Chronos\n",
    "        - ground_truth_labels: Anomaly labels\n",
    "        - actual_future_values: Values to compare against predictions\n",
    "    \"\"\"\n",
    "    # Read CSV\n",
    "    df = pd.read_csv(dataset_path, header=0, index_col=None)\n",
    "    \n",
    "    # Remove label from data\n",
    "    df_clean = df.drop(columns=[df.columns[-1]]).copy()\n",
    "    \n",
    "    # Create Chronos-compatible DataFrame\n",
    "    df_chronos = pd.DataFrame()\n",
    "    df_chronos['timestamp'] = get_timestamp(periods=len(df_clean))\n",
    "    df_chronos['item_id'] = 0  # Single time series\n",
    "    df_chronos[df.columns[0]] = df_clean[df.columns[0]].values\n",
    "    \n",
    "    return df_chronos, df[df.columns[-1]].values, df.columns[0]\n",
    "\n",
    "\n",
    "def make_predictions_sliding_window(time_series_df: pd.DataFrame,pipeline: Chronos2Pipeline,target_col: str,context_length: int = 100,prediction_length: int = 1,step_size: int = 1,batch_size: int = 32,\n",
    "                                    quantile_levels: list[float] = [0.01, 0.05, 0.1, 0.5, 0.9, 0.95, 0.99]):\n",
    "    \"\"\"\n",
    "    Generate predictions using sliding window approach\n",
    "    \n",
    "    Args:\n",
    "        time_series_df: DataFrame with columns [timestamp, item_id, target]\n",
    "        pipeline: Chronos2Pipeline instance\n",
    "        target_col: Name of target column\n",
    "        context_length: Number of historical points for context\n",
    "        prediction_length: Number of steps to forecast\n",
    "        step_size: Stride of sliding window\n",
    "        batch_size: Batch size for inference\n",
    "    \n",
    "    Returns:\n",
    "        predictions_df: DataFrame with predictions and quantiles\n",
    "        prediction_indices: Indices in original series corresponding to each prediction\n",
    "    \"\"\"\n",
    "    \n",
    "    predictions_list, prediction_indices = [], []\n",
    "    \n",
    "    # Prepare context-future pairs\n",
    "    contexts, futures, indices = [], [], []\n",
    "    \n",
    "    #create sliding windows \n",
    "    idx = context_length\n",
    "    while idx + prediction_length <= len(time_series_df):\n",
    "        # Extract context\n",
    "        contexts.append(time_series_df.iloc[idx - context_length:idx].copy())\n",
    "        \n",
    "        # Extract future metadata (timestamp, item_id for next step)\n",
    "        futures.append(time_series_df[['timestamp', 'item_id']].iloc[idx:idx + prediction_length].copy())\n",
    "        \n",
    "        indices.append(idx)\n",
    "        idx += step_size\n",
    "    \n",
    "    print(f\"Total prediction windows: {len(contexts)}\")\n",
    "    \n",
    "    batch_range = range(0, len(contexts), batch_size)\n",
    "    for batch_start in tqdm.tqdm(batch_range, desc=\"Processing prediction batches\", leave=False):\n",
    "        batch_end = min(batch_start + batch_size, len(contexts)) #l'ultimo batch potrebbe essere più piccolo\n",
    "        batch_contexts = contexts[batch_start:batch_end]\n",
    "        \n",
    "        try:\n",
    "            # Combine contexts with unique item_id\n",
    "            combined_contexts = []\n",
    "            for i, ctx in enumerate(batch_contexts):\n",
    "                ctx_copy = ctx.copy()\n",
    "                ctx_copy['item_id'] = i\n",
    "                combined_contexts.append(ctx_copy)\n",
    "            \n",
    "            # Combine futures with matching item_id\n",
    "            combined_futures = []\n",
    "            for i, fut in enumerate(futures[batch_start:batch_end]):\n",
    "                fut_copy = fut.copy()\n",
    "                fut_copy['item_id'] = i\n",
    "                combined_futures.append(fut_copy)\n",
    "            \n",
    "            # Make predictions\n",
    "            pred_df = pipeline.predict_df(\n",
    "                df=pd.concat(combined_contexts, ignore_index=True),\n",
    "                future_df=pd.concat(combined_futures, ignore_index=True),\n",
    "                target=target_col,\n",
    "                prediction_length=prediction_length,\n",
    "                quantile_levels=quantile_levels,  # Use multiple quantiles\n",
    "                cross_learning=False,\n",
    "                batch_size=len(batch_contexts),\n",
    "            )\n",
    "            \n",
    "            predictions_list.append(pred_df)\n",
    "            \n",
    "            # Map each prediction row to its corresponding timestep index\n",
    "            # When prediction_length > 1, each context window produces prediction_length predictions\n",
    "            for start_idx in indices[batch_start:batch_end]:\n",
    "                for pred_step in range(prediction_length):\n",
    "                    prediction_indices.append(start_idx + pred_step)\n",
    "            \n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch starting at index {batch_start}: {e}\")\n",
    "\n",
    "    if predictions_list:\n",
    "        return pd.concat(predictions_list, ignore_index=True), np.array(prediction_indices)\n",
    "    return pd.DataFrame(), np.array(prediction_indices)\n",
    "\n",
    "\n",
    "def detect_anomalies_reconstruction_error(predictions_df: pd.DataFrame,actual_values: np.ndarray, thresholds_percentile:list[list[float]] = \n",
    "                                            [[0.2, 0.8], [0.01, 0.99],[0.05, 0.95],[0.025, 0.975],[0.1, 0.9]]):\n",
    "    \"\"\"\n",
    "    Detect anomalies using reconstruction error (prediction error)\n",
    "    \n",
    "    Args:\n",
    "        predictions_df: DataFrame with '0.5' column (median predictions)\n",
    "        actual_values: Actual observed values\n",
    "        thresholds_percentile: List of [lower_percentile, upper_percentile] pairs for thresholding\n",
    "        \n",
    "    Returns:\n",
    "        anomaly_labels: Binary array (0=normal, 1=anomaly)\n",
    "        reconstruction_errors: Absolute errors\n",
    "        threshold: Used threshold\n",
    "    \"\"\"\n",
    "    return ([((actual_values < predictions_df[str(q_low)].to_numpy()) | (actual_values > predictions_df[str(q_high)].to_numpy())).astype(np.int8) \n",
    "            for q_low, q_high in thresholds_percentile], thresholds_percentile)\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_dataset(dataset_path: str,pipeline: Chronos2Pipeline,context_length: int = 100,\n",
    "        thresholds_percentile: list[list[float]] = [[0.2, 0.8], [0.01, 0.99], [0.05, 0.95], [0.025, 0.975],[0.1, 0.9]],\n",
    "        step_size: int = 1,batch_size: int = 32, prediction_length: int = 1):\n",
    "    \"\"\"\n",
    "    Complete evaluation pipeline for a single dataset\n",
    "    \"\"\"\n",
    "    print(f\"Processing: {os.path.basename(dataset_path)}\")\n",
    "    \n",
    "    # Prepare data\n",
    "    time_series_df, ground_truth_labels, target_col = prepare_data_for_chronos(dataset_path)\n",
    "    \n",
    "    print(f\"Ground truth anomaly rate: {np.mean(ground_truth_labels):.2%}\")\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions_df, prediction_indices = make_predictions_sliding_window(\n",
    "        time_series_df=time_series_df,\n",
    "        pipeline=pipeline,\n",
    "        target_col=target_col,\n",
    "        context_length=context_length,\n",
    "        prediction_length=prediction_length,\n",
    "        step_size=step_size,\n",
    "        batch_size=batch_size,\n",
    "        quantile_levels=[t for v in thresholds_percentile for t in v] + [0.5] \n",
    "    )\n",
    "    \n",
    "    if not len(predictions_df):\n",
    "        print(\"No predictions generated!\")\n",
    "        return None\n",
    "    \n",
    "    # Detect anomalies using reconstruction error\n",
    "    predictedAnomalies, th  = detect_anomalies_reconstruction_error(\n",
    "        predictions_df=predictions_df,\n",
    "        actual_values=time_series_df[target_col].iloc[prediction_indices].values,\n",
    "        thresholds_percentile=thresholds_percentile\n",
    "    )\n",
    "\n",
    "    # Calculate metrics\n",
    "    return {\n",
    "        'file': os.path.basename(dataset_path),\n",
    "        \"thresholds\": th,\n",
    "        'metrics':[{\n",
    "            **get_metrics(predicted, ground_truth_labels[prediction_indices]),\n",
    "            'accuracy': float(accuracy_score(ground_truth_labels[prediction_indices], predicted)),\n",
    "            'precision': float(precision_score(ground_truth_labels[prediction_indices], predicted, zero_division=0)),\n",
    "            'recall': float(recall_score(ground_truth_labels[prediction_indices], predicted, zero_division=0)),\n",
    "            'f1_score': float(f1_score(ground_truth_labels[prediction_indices], predicted, zero_division=0)),\n",
    "            'confusion_matrix': confusion_matrix(ground_truth_labels[prediction_indices], predicted).tolist(),\n",
    "            'thresholds': t} for t, predicted in zip(th, predictedAnomalies)]\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7007792d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Processing: 074_WSD_id_46_WebService_tr_990_1st_1090.csv\n",
      "Ground truth anomaly rate: 1.59%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Main execution\"\"\"\n",
    "\n",
    "# Configuration\n",
    "data_path = \"./TSB-AD-U\"\n",
    "\n",
    "out_initial_path = \"./results/univariate/\"\n",
    "\n",
    "os.makedirs(out_initial_path, exist_ok=True)\n",
    "\n",
    "# Parameters\n",
    "context_length = 100\n",
    "thresholds_percentile = [[0.2, 0.8], [0.1, 0.9], [0.05, 0.95], [0.025, 0.975], [0.01, 0.99]]\n",
    "step_size = 1\n",
    "batch_size = 256\n",
    "prediction_length = 1\n",
    "\n",
    "pipeline = get_pipeline(device='cuda')\n",
    "print(f\"Using device: {next(pipeline.model.parameters()).device}\")\n",
    "\n",
    "if os.path.exists(os.path.join(out_initial_path, \"results.json\")):\n",
    "    with open(os.path.join(out_initial_path, \"results.json\"), 'r', encoding='utf-8') as f:\n",
    "        existing_results = json_load(f)\n",
    "else:\n",
    "    existing_results = {}\n",
    "\n",
    "# 1. Ottieni la lista ordinata di tutti i file CSV\n",
    "all_files = sorted([f for f in os.listdir(data_path) if f.endswith('.csv')])\n",
    "\n",
    "# 2. SELEZIONA DAL FILE 73 IN POI \n",
    "files_to_process = all_files[73:]\n",
    "\n",
    "# Process datasets\n",
    "for filename in files_to_process:\n",
    "    if filename in existing_results:\n",
    "        print(f\"Skipping already processed file: {filename}\")\n",
    "        continue\n",
    "    \n",
    "    result = evaluate_dataset(\n",
    "        os.path.join(data_path, filename),\n",
    "        pipeline=pipeline,\n",
    "        context_length=context_length,\n",
    "        thresholds_percentile=thresholds_percentile,\n",
    "        step_size=step_size,\n",
    "        batch_size=batch_size,\n",
    "        prediction_length=prediction_length\n",
    "    )\n",
    "    \n",
    "    if result is not None:\n",
    "        with open(os.path.join(out_initial_path, \"results.json\"), 'w', encoding='utf-8') as f:\n",
    "            existing_results[filename] = {**result, 'context_length': context_length, 'prediction_length': prediction_length,\n",
    "                        'step_size': step_size, \"batch_size\": batch_size}\n",
    "            json_dump(existing_results, f, indent=4)\n",
    "            print(f\"Results saved for {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc2d551",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
